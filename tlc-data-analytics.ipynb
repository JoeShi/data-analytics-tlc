{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TLC Trip Data Record Analysis\n",
    "\n",
    "This is the notebook used to analyze the TLC Trip Data Record. In this notebook, we will enrich the data with the New York weather data, public holiday data, drop unnecessary columns, and create new columns for further analysis.\n",
    "\n",
    "We will save the data in Amazon S3 bucket partioned by year, month, day, hour and vehicle operater. And prepare the data to be used in visualization tool, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Print the magics in Glue Spark kernel\n",
    "%help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Launch the Glue Interactive Sessions development environment\n",
    "\n",
    "We develop the notebook in the Glue Interactive Sessions development environment. To launch the environment, follow the steps below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are already connected to a glueetl session c8b783ae-a23d-4769-99fa-79d01bd86409.\n",
      "\n",
      "No change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current idle_timeout is 2880 minutes.\n",
      "idle_timeout has been set to 2880 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are already connected to a glueetl session c8b783ae-a23d-4769-99fa-79d01bd86409.\n",
      "\n",
      "No change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Glue version to: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are already connected to a glueetl session c8b783ae-a23d-4769-99fa-79d01bd86409.\n",
      "\n",
      "No change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous worker type: G.1X\n",
      "Setting new worker type to: G.1X\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are already connected to a glueetl session c8b783ae-a23d-4769-99fa-79d01bd86409.\n",
      "\n",
      "No change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous number of workers: 4\n",
      "Setting new number of workers to: 4\n"
     ]
    }
   ],
   "source": [
    "%idle_timeout 2880\n",
    "%glue_version 4.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "# pyspark.sql.functions\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, dayofweek, date_format\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Glue Job:  False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Detect if the code is running in a Glue job\n",
    "def is_glue_job():\n",
    "    try:\n",
    "        args = getResolvedOptions(sys.argv,['JOB_NAME'])\n",
    "        print(\"JOB_NAME: \", args['JOB_NAME'])\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "print(\"Running in Glue Job: \", is_glue_job())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TLC Trip Data Record \n",
    "\n",
    "The TLC Trip Data Record is a public dataset provided by the New York City Taxi and Limousine Commission (TLC) that contains data on over 1.1 billion taxi trips in New York City. The data is stored in Amazon S3 as a CSV file with a separate file for each month and year. The data is partitioned by year and month. The data is available from 2009 to the present. The data is updated on a monthly basis.\n",
    "\n",
    "Before we start, we have already download the For-hire Vehicle (FHV) trip records from 2019.2 to 2013.6. The data is stored in the S3 bucket.\n",
    "\n",
    "### 2.1 Load Trip Record Data\n",
    "\n",
    "Load the data from S3 bucket and convert to a Spark DataFrame. The Parquet schema since 2023-02 has changed, the PULocationID & DOLocationID was INT64 before 2023-02, but INT32 after 2023-02. To make the data consistent, we will convert to INT64.\n",
    "\n",
    "To to this, we put the data in two folders in Amazon S3 bucket, `fhvhv` contains the data before 2023-02, and `fhvhv_abnormal` contains the data since 2023-02. We load the data into 2 Spark DataFrame, cast the INT32 columns to INT64, concatenate the two DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, IntegerType, BinaryType, TimestampType, LongType\n",
    "\n",
    "schema_since_2023_02 = StructType([\n",
    "    StructField(\"hvfhs_license_num\", StringType()),\n",
    "    StructField(\"dispatching_base_num\", StringType()),\n",
    "    StructField(\"request_datetime\", TimestampType()),\n",
    "    StructField(\"pickup_datetime\", TimestampType()),\n",
    "    StructField(\"dropoff_datetime\", TimestampType()),\n",
    "    StructField(\"PULocationID\", IntegerType()),\n",
    "    StructField(\"DOLocationID\", IntegerType()),\n",
    "    StructField(\"trip_miles\", DoubleType()),\n",
    "    StructField(\"trip_time\", LongType()),\n",
    "    StructField(\"base_passenger_fare\", DoubleType()),\n",
    "    StructField(\"tolls\", DoubleType()),\n",
    "    StructField(\"sales_tax\", DoubleType()),\n",
    "    StructField(\"congestion_surcharge\", DoubleType()),\n",
    "    StructField(\"tips\", DoubleType()),\n",
    "    StructField(\"driver_pay\", DoubleType()),\n",
    "    StructField(\"shared_request_flag\", StringType()),\n",
    "    StructField(\"shared_match_flag\", StringType())\n",
    "])\n",
    "\n",
    "schema_before_2023_02 = StructType([\n",
    "    StructField(\"hvfhs_license_num\", StringType()),\n",
    "    StructField(\"dispatching_base_num\", StringType()),\n",
    "    StructField(\"request_datetime\", TimestampType()),\n",
    "    StructField(\"pickup_datetime\", TimestampType()),\n",
    "    StructField(\"dropoff_datetime\", TimestampType()),\n",
    "    StructField(\"PULocationID\", LongType()),\n",
    "    StructField(\"DOLocationID\", LongType()),\n",
    "    StructField(\"trip_miles\", DoubleType()),\n",
    "    StructField(\"trip_time\", LongType()),\n",
    "    StructField(\"base_passenger_fare\", DoubleType()),\n",
    "    StructField(\"tolls\", DoubleType()),\n",
    "    StructField(\"sales_tax\", DoubleType()),\n",
    "    StructField(\"congestion_surcharge\", DoubleType()),\n",
    "    StructField(\"tips\", DoubleType()),\n",
    "    StructField(\"driver_pay\", DoubleType()),\n",
    "    StructField(\"shared_request_flag\", StringType()),\n",
    "    StructField(\"shared_match_flag\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in a development environment, read a sample\n",
      "Count before 2023-02:  7835862\n",
      "Count since 2023-02:  967020\n"
     ]
    }
   ],
   "source": [
    "# if running in Glue job, read all data, otherwise, read a sample\n",
    "if is_glue_job():\n",
    "    print(\"Running in Glue job, read all data\")\n",
    "    df_before_2023_02 = spark.read.schema(schema_before_2023_02).parquet(\"s3://qiaoshi-aws-ml/tlc/fhvhv/\")\n",
    "    df_since_2023_02 = spark.read.schema(schema_since_2023_02).parquet(\"s3://qiaoshi-aws-ml/tlc/fhvhv_abnormal/\")\n",
    "else:\n",
    "    print(\"Running in a development environment, read a sample\")\n",
    "    df_before_2023_02 = spark.read.schema(schema_before_2023_02).parquet(\"s3://qiaoshi-aws-ml/tlc/fhvhv/\").sample(False, 0.01) # 1% sample\n",
    "    df_since_2023_02 = spark.read.schema(schema_since_2023_02).parquet(\"s3://qiaoshi-aws-ml/tlc/fhvhv_abnormal/\").sample(False, 0.01) # 1% sample\n",
    "\n",
    "\n",
    "df_since_2023_02 = df_since_2023_02.withColumn(\"PULocationID\", df_since_2023_02[\"PULocationID\"].cast(\"long\"))\n",
    "df_since_2023_02 = df_since_2023_02.withColumn(\"DOLocationID\", df_since_2023_02[\"DOLocationID\"].cast(\"long\"))\n",
    "print(\"Count before 2023-02: \", df_before_2023_02.count())\n",
    "print(\"Count since 2023-02: \", df_since_2023_02.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total counts after union:  8802882\n"
     ]
    }
   ],
   "source": [
    "df_records = df_before_2023_02.union(df_since_2023_02)\n",
    "print(\"Total counts after union: \", df_records.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Transformation\n",
    "\n",
    "In this seciton, we did the following data transformation:\n",
    "\n",
    "- The column `request_datetime` contains null, we fill it with the `pickup_datetime`.\n",
    "- The column `congestion_surcharge` contains null value, we fill it with 0.\n",
    "- The column `trip_time` is null, we fill it with the difference between `dropoff_datetime` and `pickup_datetime`.\n",
    "- Convert the `hvfhs_license_num` to the rider name, Juno, Uber, Via, Lyft.\n",
    "- Convert the U.S metrics to metric system, so it is easier to understand for international audience.\n",
    "- Add columns `year`, `month`, `day`, `hour`, `weekday` based on the `request_datetime` for further partitioning the data. We will use this to join with the weather data in New York City.\n",
    "- Convert the `trip_miles` from miles to kilometers.\n",
    "\n",
    "\n",
    "The `hvfhs_license_num` is the TLC license number of the HVFHS base or business. Convert it to the well-recognized name as following.\n",
    "- HV0002: Juno\n",
    "- HV0003: Uber\n",
    "- HV0004: Via\n",
    "- HV0005: Lyft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total counts after filling null:  8802882\n",
      "+-----------------+--------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+---------+--------------------+----+----------+-------------------+-----------------+\n",
      "|hvfhs_license_num|dispatching_base_num|   request_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls|sales_tax|congestion_surcharge|tips|driver_pay|shared_request_flag|shared_match_flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+---------+--------------------+----+----------+-------------------+-----------------+\n",
      "|           HV0003|              B02884|2019-01-31 23:52:45|2019-02-01 00:03:13|2019-02-01 00:12:12|         174|         254|       2.2|      539|               6.54|  0.0|     0.58|                 0.0| 0.0|      4.25|                  Y|                Y|\n",
      "|           HV0003|              B02871|2019-02-01 00:21:00|2019-02-01 00:26:00|2019-02-01 00:35:06|          18|         136|      1.68|      547|               4.85|  0.0|     0.43|                 0.0| 0.0|      6.89|                  N|                N|\n",
      "|           HV0005|              B02510|2019-02-01 00:51:00|2019-02-01 00:52:58|2019-02-01 01:01:17|          17|         225|      1.88|      479|               8.46|  0.0|     0.75|                 0.0| 0.0|      5.93|                  N|                Y|\n",
      "|           HV0003|              B02617|2019-02-01 00:44:35|2019-02-01 00:47:43|2019-02-01 01:10:52|          97|          22|       9.8|     1389|              27.96|  0.0|     2.48|                 0.0| 5.0|     27.23|                  N|                N|\n",
      "|           HV0003|              B02871|2019-02-01 00:36:57|2019-02-01 00:40:29|2019-02-01 01:00:27|          79|         186|      2.41|     1199|              10.06|  0.0|     0.89|                 0.0| 0.0|      13.0|                  N|                N|\n",
      "+-----------------+--------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+---------+--------------------+----+----------+-------------------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# fill the request_datetime with pickup_datetime if null\n",
    "df_records = df_records.withColumn(\"request_datetime\", coalesce(df_records[\"request_datetime\"], df_records[\"pickup_datetime\"]))\n",
    "\n",
    "# if the value of congestion_surcharge is null, fill it with 0\n",
    "df_records = df_records.fillna({'congestion_surcharge': 0.0})\n",
    "print(\"Total counts after filling null: \", df_records.count())\n",
    "\n",
    "df_records.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total counts after adding trip_time:  8802882\n",
      "+-----------------+--------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+---------+--------------------+----+----------+-------------------+-----------------+\n",
      "|hvfhs_license_num|dispatching_base_num|   request_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls|sales_tax|congestion_surcharge|tips|driver_pay|shared_request_flag|shared_match_flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+---------+--------------------+----+----------+-------------------+-----------------+\n",
      "|           HV0003|              B02884|2019-01-31 23:52:45|2019-02-01 00:03:13|2019-02-01 00:12:12|         174|         254|       2.2|    539.0|               6.54|  0.0|     0.58|                 0.0| 0.0|      4.25|                  Y|                Y|\n",
      "|           HV0003|              B02871|2019-02-01 00:21:00|2019-02-01 00:26:00|2019-02-01 00:35:06|          18|         136|      1.68|    546.0|               4.85|  0.0|     0.43|                 0.0| 0.0|      6.89|                  N|                N|\n",
      "|           HV0005|              B02510|2019-02-01 00:51:00|2019-02-01 00:52:58|2019-02-01 01:01:17|          17|         225|      1.88|    499.0|               8.46|  0.0|     0.75|                 0.0| 0.0|      5.93|                  N|                Y|\n",
      "|           HV0003|              B02617|2019-02-01 00:44:35|2019-02-01 00:47:43|2019-02-01 01:10:52|          97|          22|       9.8|   1389.0|              27.96|  0.0|     2.48|                 0.0| 5.0|     27.23|                  N|                N|\n",
      "|           HV0003|              B02871|2019-02-01 00:36:57|2019-02-01 00:40:29|2019-02-01 01:00:27|          79|         186|      2.41|   1198.0|              10.06|  0.0|     0.89|                 0.0| 0.0|      13.0|                  N|                N|\n",
      "+-----------------+--------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+---------+--------------------+----+----------+-------------------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Add column `trip_time` from `dropoff_datetime` and `pickup_datetime`.\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "df_records = df_records.withColumn(\"trip_time\", (unix_timestamp(df_records[\"dropoff_datetime\"]) - unix_timestamp(df_records[\"pickup_datetime\"])).cast(\"double\"))\n",
    "print(\"Total counts after adding trip_time: \", df_records.count())\n",
    "df_records.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total counts after adding rider:  8802882\n",
      "+-----------------+--------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+---------+--------------------+----+----------+-------------------+-----------------+-----+\n",
      "|hvfhs_license_num|dispatching_base_num|   request_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls|sales_tax|congestion_surcharge|tips|driver_pay|shared_request_flag|shared_match_flag|rider|\n",
      "+-----------------+--------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+---------+--------------------+----+----------+-------------------+-----------------+-----+\n",
      "|           HV0003|              B02884|2019-01-31 23:52:45|2019-02-01 00:03:13|2019-02-01 00:12:12|         174|         254|       2.2|    539.0|               6.54|  0.0|     0.58|                 0.0| 0.0|      4.25|                  Y|                Y| Uber|\n",
      "|           HV0003|              B02871|2019-02-01 00:21:00|2019-02-01 00:26:00|2019-02-01 00:35:06|          18|         136|      1.68|    546.0|               4.85|  0.0|     0.43|                 0.0| 0.0|      6.89|                  N|                N| Uber|\n",
      "|           HV0005|              B02510|2019-02-01 00:51:00|2019-02-01 00:52:58|2019-02-01 01:01:17|          17|         225|      1.88|    499.0|               8.46|  0.0|     0.75|                 0.0| 0.0|      5.93|                  N|                Y| Lyft|\n",
      "|           HV0003|              B02617|2019-02-01 00:44:35|2019-02-01 00:47:43|2019-02-01 01:10:52|          97|          22|       9.8|   1389.0|              27.96|  0.0|     2.48|                 0.0| 5.0|     27.23|                  N|                N| Uber|\n",
      "|           HV0003|              B02871|2019-02-01 00:36:57|2019-02-01 00:40:29|2019-02-01 01:00:27|          79|         186|      2.41|   1198.0|              10.06|  0.0|     0.89|                 0.0| 0.0|      13.0|                  N|                N| Uber|\n",
      "+-----------------+--------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+---------+--------------------+----+----------+-------------------+-----------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# add a new column to indicate the rider\n",
    "df_records = df_records.withColumn(\"rider\", \n",
    "                        when(df_records[\"hvfhs_license_num\"] == \"HV0002\", \"Juno\")\n",
    "                        .when(df_records[\"hvfhs_license_num\"] == \"HV0003\", \"Uber\")\n",
    "                        .when(df_records[\"hvfhs_license_num\"] == \"HV0004\", \"Via\")\n",
    "                        .when(df_records[\"hvfhs_license_num\"] == \"HV0005\", \"Lyft\")\n",
    "                        .otherwise(\"Unknown\")  # Optional: for unmapped values\n",
    "                        )\n",
    "print(\"Total counts after adding rider: \", df_records.count())\n",
    "df_records.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Add columns `year`, `month`, `day`, `hour`, `weekday` from `request_datetime`.\n",
    "df_records = df_records.withColumn(\"year\", year(df_records[\"request_datetime\"]))\n",
    "df_records = df_records.withColumn(\"month\", month(df_records[\"request_datetime\"]))\n",
    "df_records = df_records.withColumn(\"day\", dayofmonth(df_records[\"request_datetime\"]))\n",
    "df_records = df_records.withColumn(\"hour\", hour(df_records[\"request_datetime\"]))\n",
    "df_records = df_records.withColumn(\"weekday_n\", dayofweek(df_records[\"request_datetime\"]))\n",
    "df_records = df_records.withColumn(\"weekday\", date_format(df_records[\"request_datetime\"], \"EEEE\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the trip_miles from miles to kilometers\n",
    "df_records = df_records.withColumn(\"trip_km\", df_records[\"trip_miles\"] * 1.60934)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Glue job, skip writing to S3 bucket.\n"
     ]
    }
   ],
   "source": [
    "# Write the data to the S3 bucket in parquet format partitioned by year, month, day if running in Glue job\n",
    "if is_glue_job():\n",
    "    print(\"Writing to S3 bucket...\")\n",
    "    df_records.write.mode(\"append\").format(\"parquet\").partitionBy(\"year\", \"month\", \"day\").save(\"s3://qiaoshi-aws-ml/tlc/fhvhv_partitioned/\")\n",
    "    print(\"Done writing to S3 bucket.\")\n",
    "else:\n",
    "    print(\"Not running in Glue job, skip writing to S3 bucket.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. New York Weather data\n",
    "\n",
    "Weather is a very important factor of the taxi trip. We use the NY_Weather data to join with the TLC Trip Data Record. The data is downloaded from [Visual Crossing](https://www.visualcrossing.com/). \n",
    "\n",
    "We download the weather data from 2019.1 to 2023.6, and store the data in the Amazon S3 bucket. After that, we use the data to join with the TLC Trip Data Record.\n",
    "\n",
    "### 3.1 Data Column Definitions\n",
    "\n",
    "- name: name of the weather station\n",
    "- datetime: date and time of the weather record\n",
    "- temp: temperature in Fahrenheit\n",
    "- feelslike: feels like temperature in Fahrenheit\n",
    "- dewp: dew point in Fahrenheit\n",
    "- humid: relative humidity\n",
    "- precip: precipitation in inches\n",
    "- precipprob: probability of precipitation\n",
    "- snow: snowfall in inches\n",
    "- snowdepth: snow depth in inches\n",
    "- windgust: wind gust in miles per hour\n",
    "- windspeed: wind speed in miles per hour\n",
    "- winddir: wind direction in degrees\n",
    "- sealvlpressure: sea level pressure in millibars\n",
    "- cloudcover: cloud cover percentage\n",
    "- visibility: visibility in miles\n",
    "- solarradiation: solar radiation in watts per square meter\n",
    "- solarenergy: solar energy in megajoules per square meter\n",
    "- uvindex: UV index\n",
    "- severisky: severe weather risk index\n",
    "- conditions: weather conditions\n",
    "- icon: weather icon\n",
    "- stations: number of weather stations reporting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load Weather Data\n",
    "\n",
    "Since the weather data is well formated, we use Glue Crawler to create a Glue Table, and load into a Spark DataFrame.\n",
    "\n",
    "The weather data is stored in the Amazon S3 bucket, and the Glue Table name is `tlcny_weather`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "|-- name: string\n",
      "|-- datetime: string\n",
      "|-- temp: string\n",
      "|-- feelslike: string\n",
      "|-- dew: string\n",
      "|-- humidity: double\n",
      "|-- precip: double\n",
      "|-- precipprob: long\n",
      "|-- preciptype: string\n",
      "|-- snow: double\n",
      "|-- snowdepth: double\n",
      "|-- windgust: string\n",
      "|-- windspeed: double\n",
      "|-- winddir: string\n",
      "|-- sealevelpressure: double\n",
      "|-- cloudcover: double\n",
      "|-- visibility: double\n",
      "|-- solarradiation: string\n",
      "|-- solarenergy: double\n",
      "|-- uvindex: long\n",
      "|-- severerisk: string\n",
      "|-- conditions: string\n",
      "|-- icon: string\n",
      "|-- stations: string\n"
     ]
    }
   ],
   "source": [
    "dyf_weather = glueContext.create_dynamic_frame.from_catalog(database='tlc', table_name='tlcny_weather')\n",
    "dyf_weather.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of weather records:  38687\n"
     ]
    }
   ],
   "source": [
    "# Convert Dynamic DataFrame to Spark DataFrame\n",
    "df_weather = dyf_weather.toDF()\n",
    "print(\"Count of weather records: \", df_weather.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Transformation\n",
    "\n",
    "In this seciton, we did the following data transformation:\n",
    "\n",
    "1. Drop duplicated fields `name`, `dew`, `stations`, `conditions`, `severerisk`.\n",
    "2. Convert the U.S metrics to metric system, so it is easier to understand for international audience.\n",
    "3. Add columns `year`, `month`, `day`, `hour`.\n",
    "4. Remove duplicated rows.\n",
    "\n",
    "We added the `year`, `month`, `day`, `hour` fields based on the `request_datetime` in the TLC Trip Data Record.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+---------+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+-----------+\n",
      "|           datetime| temp|feelslike|humidity|precip|precipprob|preciptype|snow|snowdepth|windgust|windspeed|winddir|sealevelpressure|cloudcover|visibility|solarradiation|solarenergy|uvindex|       icon|\n",
      "+-------------------+-----+---------+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+-----------+\n",
      "|2019-02-01T00:00:00| -9.4|    -15.4|   47.32|   0.0|         0|          | 0.0|      0.0|        |     13.1|    250|          1030.9|       0.4|      16.0|             0|        0.0|      0|clear-night|\n",
      "|2019-02-01T01:00:00| -9.4|    -15.9|   47.16|   0.0|         0|          | 0.0|      0.0|    44.6|     14.9|    260|          1030.5|       0.4|      16.0|             0|        0.0|      0|clear-night|\n",
      "|2019-02-01T02:00:00| -9.4|    -15.5|   45.09|   0.0|         0|          | 0.0|      0.0|        |     13.1|    261|          1030.8|       0.4|      16.0|             0|        0.0|      0|clear-night|\n",
      "|2019-02-01T03:00:00| -9.4|    -13.7|   45.09|   0.0|         0|          | 0.0|      0.0|    42.5|      7.9|    300|          1031.1|       0.4|      16.0|             0|        0.0|      0|clear-night|\n",
      "|2019-02-01T04:00:00|-10.1|    -16.8|   49.82|   0.0|         0|          | 0.0|      0.0|    25.9|     15.0|    261|          1031.2|       0.4|      16.0|             0|        0.0|      0|clear-night|\n",
      "+-------------------+-----+---------+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Drop columns: name (location name), stations (number of stations reporting), conditions (weather conditions), severerisk (severe weather risk)\n",
    "df_weather = df_weather.drop(\"name\", \"dew\", \"stations\", \"conditions\", \"severerisk\")\n",
    "\n",
    "df_weather.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+---------+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+-----------+------+-----------+---------+-------+------------+------------+-------------+-------------+\n",
      "|           datetime| temp|feelslike|humidity|precip|precipprob|preciptype|snow|snowdepth|windgust|windspeed|winddir|sealevelpressure|cloudcover|visibility|solarradiation|solarenergy|uvindex|       icon|temp_c|feelslike_c|precip_cm|snow_cm|snowdepth_cm|windgust_mps|windspeed_mps|visibility_km|\n",
      "+-------------------+-----+---------+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+-----------+------+-----------+---------+-------+------------+------------+-------------+-------------+\n",
      "|2019-02-01T00:00:00| -9.4|    -15.4|   47.32|   0.0|         0|          | 0.0|      0.0|        |     13.1|    250|          1030.9|       0.4|      16.0|             0|        0.0|      0|clear-night| -23.0|      -26.3|      0.0|    0.0|         0.0|        null|          5.9|         25.7|\n",
      "|2019-02-01T01:00:00| -9.4|    -15.9|   47.16|   0.0|         0|          | 0.0|      0.0|    44.6|     14.9|    260|          1030.5|       0.4|      16.0|             0|        0.0|      0|clear-night| -23.0|      -26.6|      0.0|    0.0|         0.0|        19.9|          6.7|         25.7|\n",
      "|2019-02-01T02:00:00| -9.4|    -15.5|   45.09|   0.0|         0|          | 0.0|      0.0|        |     13.1|    261|          1030.8|       0.4|      16.0|             0|        0.0|      0|clear-night| -23.0|      -26.4|      0.0|    0.0|         0.0|        null|          5.9|         25.7|\n",
      "|2019-02-01T03:00:00| -9.4|    -13.7|   45.09|   0.0|         0|          | 0.0|      0.0|    42.5|      7.9|    300|          1031.1|       0.4|      16.0|             0|        0.0|      0|clear-night| -23.0|      -25.4|      0.0|    0.0|         0.0|        19.0|          3.5|         25.7|\n",
      "|2019-02-01T04:00:00|-10.1|    -16.8|   49.82|   0.0|         0|          | 0.0|      0.0|    25.9|     15.0|    261|          1031.2|       0.4|      16.0|             0|        0.0|      0|clear-night| -23.4|      -27.1|      0.0|    0.0|         0.0|        11.6|          6.7|         25.7|\n",
      "+-------------------+-----+---------+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+-----------+------+-----------+---------+-------+------------+------------+-------------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Convert the U.S metrics to metric system, so it is easier to understand for international audience\n",
    "df_weather = df_weather.withColumn(\"temp_c\", round((df_weather[\"temp\"] - 32) * 5/9, 1))\n",
    "df_weather = df_weather.withColumn(\"feelslike_c\", round((df_weather[\"feelslike\"] - 32) * 5/9, 1))\n",
    "df_weather = df_weather.withColumn(\"precip_cm\", round(df_weather[\"precip\"] * 0.3048 * 100, 1)) # centimeters\n",
    "df_weather = df_weather.withColumn(\"snow_cm\", round(df_weather[\"snow\"] * 0.3048 * 100, 1)) # centimeters\n",
    "df_weather = df_weather.withColumn(\"snowdepth_cm\", round(df_weather[\"snowdepth\"] * 0.3048 * 100, 1)) #centimeters\n",
    "df_weather = df_weather.withColumn(\"windgust_mps\", round(df_weather[\"windgust\"] * 0.44704, 1))  # meters per second\n",
    "df_weather = df_weather.withColumn(\"windspeed_mps\", round(df_weather[\"windspeed\"] * 0.44704, 1)) # meters per second\n",
    "df_weather = df_weather.withColumn(\"visibility_km\", round(df_weather[\"visibility\"] * 1.60934, 1)) # kilometers\n",
    "\n",
    "df_weather.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+---------+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+-----------+------+-----------+---------+-------+------------+------------+-------------+-------------+----+-----+---+----+\n",
      "|           datetime| temp|feelslike|humidity|precip|precipprob|preciptype|snow|snowdepth|windgust|windspeed|winddir|sealevelpressure|cloudcover|visibility|solarradiation|solarenergy|uvindex|       icon|temp_c|feelslike_c|precip_cm|snow_cm|snowdepth_cm|windgust_mps|windspeed_mps|visibility_km|year|month|day|hour|\n",
      "+-------------------+-----+---------+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+-----------+------+-----------+---------+-------+------------+------------+-------------+-------------+----+-----+---+----+\n",
      "|2019-02-01T00:00:00| -9.4|    -15.4|   47.32|   0.0|         0|          | 0.0|      0.0|        |     13.1|    250|          1030.9|       0.4|      16.0|             0|        0.0|      0|clear-night| -23.0|      -26.3|      0.0|    0.0|         0.0|        null|          5.9|         25.7|2019|    2|  1|   0|\n",
      "|2019-02-01T01:00:00| -9.4|    -15.9|   47.16|   0.0|         0|          | 0.0|      0.0|    44.6|     14.9|    260|          1030.5|       0.4|      16.0|             0|        0.0|      0|clear-night| -23.0|      -26.6|      0.0|    0.0|         0.0|        19.9|          6.7|         25.7|2019|    2|  1|   1|\n",
      "|2019-02-01T02:00:00| -9.4|    -15.5|   45.09|   0.0|         0|          | 0.0|      0.0|        |     13.1|    261|          1030.8|       0.4|      16.0|             0|        0.0|      0|clear-night| -23.0|      -26.4|      0.0|    0.0|         0.0|        null|          5.9|         25.7|2019|    2|  1|   2|\n",
      "|2019-02-01T03:00:00| -9.4|    -13.7|   45.09|   0.0|         0|          | 0.0|      0.0|    42.5|      7.9|    300|          1031.1|       0.4|      16.0|             0|        0.0|      0|clear-night| -23.0|      -25.4|      0.0|    0.0|         0.0|        19.0|          3.5|         25.7|2019|    2|  1|   3|\n",
      "|2019-02-01T04:00:00|-10.1|    -16.8|   49.82|   0.0|         0|          | 0.0|      0.0|    25.9|     15.0|    261|          1031.2|       0.4|      16.0|             0|        0.0|      0|clear-night| -23.4|      -27.1|      0.0|    0.0|         0.0|        11.6|          6.7|         25.7|2019|    2|  1|   4|\n",
      "+-------------------+-----+---------+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+-----------+------+-----------+---------+-------+------------+------------+-------------+-------------+----+-----+---+----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Add columns year, month, day, hour based on field datetime\n",
    "\n",
    "df_weather = df_weather.withColumn(\"year\", year(df_weather[\"datetime\"]))\n",
    "df_weather = df_weather.withColumn(\"month\", month(df_weather[\"datetime\"]))\n",
    "df_weather = df_weather.withColumn(\"day\", dayofmonth(df_weather[\"datetime\"]))\n",
    "df_weather = df_weather.withColumn(\"hour\", hour(df_weather[\"datetime\"]))\n",
    "\n",
    "df_weather.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+-----+\n",
      "|year|month|day|hour|count|\n",
      "+----+-----+---+----+-----+\n",
      "|2019|   11|  3|   1|    2|\n",
      "|2022|   11|  6|   1|    2|\n",
      "|2020|   11|  1|   1|    2|\n",
      "|2021|   11|  7|   1|    2|\n",
      "+----+-----+---+----+-----+\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "duplicated_rows = df_weather.groupBy(\"year\", \"month\", \"day\", \"hour\").count().filter(col(\"count\") > 1)\n",
    "duplicated_rows.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count before removing duplicated records:  38687\n",
      "Count after removing duplicated records:  38683\n"
     ]
    }
   ],
   "source": [
    "# There is duplicated records in weather data, remove the duplicated records\n",
    "print(\"Count before removing duplicated records: \", df_weather.count())\n",
    "df_weather = df_weather.dropDuplicates([\"year\", \"month\", \"day\", \"hour\"])\n",
    "print(\"Count after removing duplicated records: \", df_weather.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Public holidays in New York City\n",
    "\n",
    "We used ChatGPT to get the public holidays in New York City in from 2019 ~ 2023. The following is the code we used to get the public holidays, and ask it to output in CSV format. And save the result, and upload it to the S3 bucket.\n",
    "\n",
    "```\n",
    "List all public holidays in New York from 2019 to 2023. Output in CSV format. Here is an example:\n",
    "\n",
    "<example>\n",
    "Year,Month,Day,Holiday\n",
    "2019,1,1,New Year's Day\n",
    "2019,1,21,Martin Luther King Jr. Day\n",
    "</example>\n",
    "```\n",
    "\n",
    "Command to upload to the Amazon S3 bucket, and use AWS Glue Crawler to create a table\n",
    "```\n",
    "aws s3 cp ~/Developer/sjtu/data-analytics/data/holidays_ny.csv s3://qiaoshi-aws-ml/tlc/holidays/ny.csv\n",
    "```\n",
    "\n",
    "\n",
    "### 3.1 Read the NY_Weather data from S3 bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------------------+\n",
      "|year|month|day|             holiday|\n",
      "+----+-----+---+--------------------+\n",
      "|2019|    1|  1|      New Year's Day|\n",
      "|2019|    1| 21|Martin Luther Kin...|\n",
      "|2019|    2| 18|     Presidents' Day|\n",
      "|2019|    5| 27|        Memorial Day|\n",
      "|2019|    7|  4|    Independence Day|\n",
      "+----+-----+---+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "dyf_holidays = glueContext.create_dynamic_frame.from_catalog(database='tlc', table_name='holidays')\n",
    "\n",
    "# Convert Dynamic DataFrame to Spark DataFrame\n",
    "df_holidays = dyf_holidays.toDF()\n",
    "\n",
    "df_holidays.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TLC Trip Data Record Enrichment, Cleanup & Exploration\n",
    "\n",
    "In this section, we will enrich the TLC Trip Data Record with the weather data and public holidays data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Enrich with Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total records before join with weather data:  8802882\n",
      "total records after join with weather data:  8805238\n",
      "+----+-----+---+----+-----------------+--------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+---------+--------------------+----+----------+-------------------+-----------------+-----+---------+--------+------------------+-------------------+----+---------+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+-----------+------+-----------+---------+-------+------------+------------+-------------+-------------+\n",
      "|year|month|day|hour|hvfhs_license_num|dispatching_base_num|   request_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls|sales_tax|congestion_surcharge|tips|driver_pay|shared_request_flag|shared_match_flag|rider|weekday_n| weekday|           trip_km|           datetime|temp|feelslike|humidity|precip|precipprob|preciptype|snow|snowdepth|windgust|windspeed|winddir|sealevelpressure|cloudcover|visibility|solarradiation|solarenergy|uvindex|       icon|temp_c|feelslike_c|precip_cm|snow_cm|snowdepth_cm|windgust_mps|windspeed_mps|visibility_km|\n",
      "+----+-----+---+----+-----------------+--------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+---------+--------------------+----+----------+-------------------+-----------------+-----+---------+--------+------------------+-------------------+----+---------+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+-----------+------+-----------+---------+-------+------------+------------+-------------+-------------+\n",
      "|2019|    2| 28|  23|           HV0003|              B02835|2019-02-28 23:51:55|2019-03-01 00:09:06|2019-03-01 00:45:05|         209|         191|     17.54|   2159.0|              33.93|  0.0|     3.03|                0.75| 0.0|     37.06|                  Y|                N| Uber|        5|Thursday|28.227823599999997|2019-02-28T23:00:00|   0|       -5|   43.85|   0.0|         0| rain,snow|0.03|     0.47|        |     18.3|     50|          1026.0|       1.5|      16.0|             0|        0.0|      0|clear-night| -17.8|      -20.6|      0.0|    0.9|        14.3|        null|          8.2|         25.7|\n",
      "|2019|    2| 28|  23|           HV0003|              B02835|2019-02-28 23:56:24|2019-03-01 00:04:21|2019-03-01 00:10:25|         242|         185|      1.19|    364.0|               5.69|  0.0|     0.51|                 0.0| 0.0|      6.24|                  N|                N| Uber|        5|Thursday|         1.9151146|2019-02-28T23:00:00|   0|       -5|   43.85|   0.0|         0| rain,snow|0.03|     0.47|        |     18.3|     50|          1026.0|       1.5|      16.0|             0|        0.0|      0|clear-night| -17.8|      -20.6|      0.0|    0.9|        14.3|        null|          8.2|         25.7|\n",
      "|2019|    2| 28|  23|           HV0004|              B02800|2019-02-28 23:50:54|2019-03-01 00:00:39|2019-03-01 00:20:04|         140|         166|       4.2|   1165.0|              10.25|  0.0|     0.31|                0.75| 0.0|       0.0|                  Y|                Y|  Via|        5|Thursday|          6.759228|2019-02-28T23:00:00|   0|       -5|   43.85|   0.0|         0| rain,snow|0.03|     0.47|        |     18.3|     50|          1026.0|       1.5|      16.0|             0|        0.0|      0|clear-night| -17.8|      -20.6|      0.0|    0.9|        14.3|        null|          8.2|         25.7|\n",
      "|2019|    2| 28|  23|           HV0003|              B02764|2019-02-28 23:55:02|2019-03-01 00:00:21|2019-03-01 00:35:14|         137|         265|      19.8|   2093.0|              49.88| 5.76|     5.47|                2.75| 0.0|     48.11|                  N|                N| Uber|        5|Thursday|         31.864932|2019-02-28T23:00:00|   0|       -5|   43.85|   0.0|         0| rain,snow|0.03|     0.47|        |     18.3|     50|          1026.0|       1.5|      16.0|             0|        0.0|      0|clear-night| -17.8|      -20.6|      0.0|    0.9|        14.3|        null|          8.2|         25.7|\n",
      "|2019|    2| 28|  23|           HV0003|              B02764|2019-02-28 23:59:09|2019-03-01 00:04:37|2019-03-01 00:13:42|          33|         195|       2.8|    545.0|               9.85|  0.0|     0.87|                 0.0| 0.0|      7.58|                  N|                N| Uber|        5|Thursday| 4.506151999999999|2019-02-28T23:00:00|   0|       -5|   43.85|   0.0|         0| rain,snow|0.03|     0.47|        |     18.3|     50|          1026.0|       1.5|      16.0|             0|        0.0|      0|clear-night| -17.8|      -20.6|      0.0|    0.9|        14.3|        null|          8.2|         25.7|\n",
      "+----+-----+---+----+-----------------+--------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+---------+--------------------+----+----------+-------------------+-----------------+-----+---------+--------+------------------+-------------------+----+---------+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+-----------+------+-----------+---------+-------+------------+------------+-------------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Join the TLC Trip Data Record with the weather data\n",
    "print(\"total records before join with weather data: \", df_records.count())\n",
    "df_enriched = df_records.join(df_weather, on=['year', 'month', 'day', 'hour'], how='left')\n",
    "\n",
    "\n",
    "print(\"total records after join with weather data: \", df_enriched.count())\n",
    "df_enriched.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Enrich with Publich Holiday Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total records after join with holiday data:  8805238\n",
      "+----+-----+---+----+-----------------+--------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+---------+--------------------+----+----------+-------------------+-----------------+-----+---------+--------+------------------+-------------------+----+---------+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+-----------+------+-----------+---------+-------+------------+------------+-------------+-------------+-------+----------+\n",
      "|year|month|day|hour|hvfhs_license_num|dispatching_base_num|   request_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls|sales_tax|congestion_surcharge|tips|driver_pay|shared_request_flag|shared_match_flag|rider|weekday_n| weekday|           trip_km|           datetime|temp|feelslike|humidity|precip|precipprob|preciptype|snow|snowdepth|windgust|windspeed|winddir|sealevelpressure|cloudcover|visibility|solarradiation|solarenergy|uvindex|       icon|temp_c|feelslike_c|precip_cm|snow_cm|snowdepth_cm|windgust_mps|windspeed_mps|visibility_km|holiday|is_holiday|\n",
      "+----+-----+---+----+-----------------+--------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+---------+--------------------+----+----------+-------------------+-----------------+-----+---------+--------+------------------+-------------------+----+---------+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+-----------+------+-----------+---------+-------+------------+------------+-------------+-------------+-------+----------+\n",
      "|2019|    2| 28|  23|           HV0003|              B02835|2019-02-28 23:51:55|2019-03-01 00:09:06|2019-03-01 00:45:05|         209|         191|     17.54|   2159.0|              33.93|  0.0|     3.03|                0.75| 0.0|     37.06|                  Y|                N| Uber|        5|Thursday|28.227823599999997|2019-02-28T23:00:00|   0|       -5|   43.85|   0.0|         0| rain,snow|0.03|     0.47|        |     18.3|     50|          1026.0|       1.5|      16.0|             0|        0.0|      0|clear-night| -17.8|      -20.6|      0.0|    0.9|        14.3|        null|          8.2|         25.7|   null|     false|\n",
      "|2019|    2| 28|  23|           HV0003|              B02835|2019-02-28 23:56:24|2019-03-01 00:04:21|2019-03-01 00:10:25|         242|         185|      1.19|    364.0|               5.69|  0.0|     0.51|                 0.0| 0.0|      6.24|                  N|                N| Uber|        5|Thursday|         1.9151146|2019-02-28T23:00:00|   0|       -5|   43.85|   0.0|         0| rain,snow|0.03|     0.47|        |     18.3|     50|          1026.0|       1.5|      16.0|             0|        0.0|      0|clear-night| -17.8|      -20.6|      0.0|    0.9|        14.3|        null|          8.2|         25.7|   null|     false|\n",
      "|2019|    2| 28|  23|           HV0004|              B02800|2019-02-28 23:50:54|2019-03-01 00:00:39|2019-03-01 00:20:04|         140|         166|       4.2|   1165.0|              10.25|  0.0|     0.31|                0.75| 0.0|       0.0|                  Y|                Y|  Via|        5|Thursday|          6.759228|2019-02-28T23:00:00|   0|       -5|   43.85|   0.0|         0| rain,snow|0.03|     0.47|        |     18.3|     50|          1026.0|       1.5|      16.0|             0|        0.0|      0|clear-night| -17.8|      -20.6|      0.0|    0.9|        14.3|        null|          8.2|         25.7|   null|     false|\n",
      "+----+-----+---+----+-----------------+--------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+---------+--------------------+----+----------+-------------------+-----------------+-----+---------+--------+------------------+-------------------+----+---------+--------+------+----------+----------+----+---------+--------+---------+-------+----------------+----------+----------+--------------+-----------+-------+-----------+------+-----------+---------+-------+------------+------------+-------------+-------------+-------+----------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# Join the TLC Trip Data Record with the public holiday data\n",
    "df_enriched = df_enriched.join(df_holidays, on=['year', 'month', 'day'], how='left').dropDuplicates()\n",
    "print(\"total records after join with holiday data: \", df_enriched.count())\n",
    "\n",
    "# Add a column is_holiday of type boolean\n",
    "df_enriched = df_enriched.withColumn(\"is_holiday\", df_enriched[\"holiday\"].isNotNull())\n",
    "print(\"total records after adding is_holiday column: \", df_enriched.count())\n",
    "\n",
    "df_enriched.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Data Cleanup\n",
    "\n",
    "There is duplicated columns in the DataFrame, we will remove some duplicated records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# drop duplicated columns\n",
    "df_enriched = df_enriched.drop(\"hvfhs_license_num\", \"datetime\", \"temp\", \"feelslike\", \"precip\", \"snow\", \"snowdepth\", \"windgust\", \"windspeed\", \"visibility\", \"trip_miles\")\n",
    "\n",
    "df_enriched.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Exploration\n",
    "\n",
    "We conduct a draft analysis of the data. We will use the data to answer the following questions:\n",
    "\n",
    "- Is there any relevance between the weather and the taxi trip?\n",
    "- What is the relevance between each columns?\n",
    "- What is the busiest day of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Column Relevance\n",
    "\n",
    "Find the relevance among each columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Compute the correlation matrix\n",
    "# corr = df_enriched.corr()\n",
    "\n",
    "# # Plot the correlation matrix as a heatmap\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# sns.heatmap(corr, cmap='coolwarm', annot=True, fmt=\".2f\")\n",
    "# plt.title(\"Correlation Matrix\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 XXXX\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the enriched TLC Trip Data Record\n",
    "\n",
    "After the ETL, we save the enriched TLC Trip Data Record to the S3 bucket. We will use the data to visualize the data in BI tools (e.g., Amazon QuickSight), and build the machine learning model in the Amazon SageMaker.\n",
    "\n",
    "We will try both zero-code machine learning using Amazon SageMaker Canvas, and code-based machine learning using Amazon SageMaker Studio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Count of records before saving: \", df_enriched.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Save the data to Amazon S3 bucket, partitioned by year, month, day\n",
    "df_enriched.write.partitionBy(\"year\", \"month\", \"day).parquet(\"s3://qiaoshi-aws-ml/tlc/fhvhv_final/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Commit the Job to AWS Glue, complete the job.\n",
    "job.commit()\n",
    "\n",
    "print('Data processing completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Finance Data\n",
    "\n",
    "Uber, and Lyft has been public companies for a while. We can get the finance data from the public market. We can use the finance data to analyze the financial performance of the companies, and compare with the TLC Trip Data Record.\n",
    "\n",
    "As of today, Uber, Lyft has never split the stock, Juno, Via are not public companies. We will use the stock price to analyze the financial performance of the companies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TLC Trip Data Record Analysis\n",
    "\n",
    "This is the notebook used to analyze the TLC Trip Data Record. In this notebook, we will enrich the data with the New York weather data, public holiday data, drop unnecessary columns, and create new columns for further analysis.\n",
    "\n",
    "We will save the data in Amazon S3 bucket partioned by year, month, day, hour and vehicle operater. And prepare the data to be used in visualization tool, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the magics in Glue Spark kernel\n",
    "%help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Launch the Glue Interactive Sessions development environment\n",
    "\n",
    "To use this notebook, you must run the AWS Glue environment.\n",
    "\n",
    "We develop the notebook in the Glue Interactive Sessions development environment. To launch the environment, follow the steps below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%idle_timeout 2880\n",
    "%glue_version 4.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "# pyspark.sql.functions\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, dayofweek, date_format\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Detect if the code is running in a Glue job\n",
    "def is_glue_job():\n",
    "    try:\n",
    "        args = getResolvedOptions(sys.argv,['JOB_NAME'])\n",
    "        print(\"JOB_NAME: \", args['JOB_NAME'])\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "print(\"Running in Glue Job: \", is_glue_job())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TLC Trip Data Record \n",
    "\n",
    "The TLC Trip Data Record is a public dataset provided by the New York City Taxi and Limousine Commission (TLC) that contains data on over 1.1 billion taxi trips in New York City. The data is stored in Amazon S3 as a CSV file with a separate file for each month and year. The data is partitioned by year and month. The data is available from 2009 to the present. The data is updated on a monthly basis.\n",
    "\n",
    "Before we start, we have already download the For-hire Vehicle (FHV) trip records from 2019.2 to 2013.6. The data is stored in the S3 bucket.\n",
    "\n",
    "### 2.1 Load Trip Record Data\n",
    "\n",
    "Load the data from S3 bucket and convert to a Spark DataFrame. The Parquet schema since 2023-02 has changed, the PULocationID & DOLocationID was INT64 before 2023-02, but INT32 after 2023-02. To make the data consistent, we will convert to INT64.\n",
    "\n",
    "To to this, we put the data in two folders in Amazon S3 bucket, `fhvhv` contains the data before 2023-02, and `fhvhv_abnormal` contains the data since 2023-02. We load the data into 2 Spark DataFrame, cast the INT32 columns to INT64, concatenate the two DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, IntegerType, BinaryType, TimestampType, LongType\n",
    "\n",
    "schema_since_2023_02 = StructType([\n",
    "    StructField(\"hvfhs_license_num\", StringType()),\n",
    "    StructField(\"dispatching_base_num\", StringType()),\n",
    "    StructField(\"request_datetime\", TimestampType()),\n",
    "    StructField(\"pickup_datetime\", TimestampType()),\n",
    "    StructField(\"dropoff_datetime\", TimestampType()),\n",
    "    StructField(\"PULocationID\", IntegerType()),\n",
    "    StructField(\"DOLocationID\", IntegerType()),\n",
    "    StructField(\"trip_miles\", DoubleType()),\n",
    "    StructField(\"trip_time\", LongType()),\n",
    "    StructField(\"base_passenger_fare\", DoubleType()),\n",
    "    StructField(\"tolls\", DoubleType()),\n",
    "    StructField(\"sales_tax\", DoubleType()),\n",
    "    StructField(\"congestion_surcharge\", DoubleType()),\n",
    "    StructField(\"tips\", DoubleType()),\n",
    "    StructField(\"driver_pay\", DoubleType()),\n",
    "    StructField(\"shared_request_flag\", StringType()),\n",
    "    StructField(\"shared_match_flag\", StringType())\n",
    "])\n",
    "\n",
    "schema_before_2023_02 = StructType([\n",
    "    StructField(\"hvfhs_license_num\", StringType()),\n",
    "    StructField(\"dispatching_base_num\", StringType()),\n",
    "    StructField(\"request_datetime\", TimestampType()),\n",
    "    StructField(\"pickup_datetime\", TimestampType()),\n",
    "    StructField(\"dropoff_datetime\", TimestampType()),\n",
    "    StructField(\"PULocationID\", LongType()),\n",
    "    StructField(\"DOLocationID\", LongType()),\n",
    "    StructField(\"trip_miles\", DoubleType()),\n",
    "    StructField(\"trip_time\", LongType()),\n",
    "    StructField(\"base_passenger_fare\", DoubleType()),\n",
    "    StructField(\"tolls\", DoubleType()),\n",
    "    StructField(\"sales_tax\", DoubleType()),\n",
    "    StructField(\"congestion_surcharge\", DoubleType()),\n",
    "    StructField(\"tips\", DoubleType()),\n",
    "    StructField(\"driver_pay\", DoubleType()),\n",
    "    StructField(\"shared_request_flag\", StringType()),\n",
    "    StructField(\"shared_match_flag\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running in Glue job, read all data, otherwise, read a sample\n",
    "if is_glue_job():\n",
    "    print(\"Running in Glue job, read all data\")\n",
    "    df_before_2023_02 = spark.read.schema(schema_before_2023_02).parquet(\"s3://qiaoshi-aws-ml/tlc/fhvhv/\")\n",
    "    df_since_2023_02 = spark.read.schema(schema_since_2023_02).parquet(\"s3://qiaoshi-aws-ml/tlc/fhvhv_abnormal/\")\n",
    "else:\n",
    "    print(\"Running in a development environment, read a sample\")\n",
    "    df_before_2023_02 = spark.read.schema(schema_before_2023_02).parquet(\"s3://qiaoshi-aws-ml/tlc/fhvhv/\").sample(False, 0.002) # 0.2% sample\n",
    "    df_since_2023_02 = spark.read.schema(schema_since_2023_02).parquet(\"s3://qiaoshi-aws-ml/tlc/fhvhv_abnormal/\").sample(False, 0.002) # 0.2% sample\n",
    "\n",
    "df_since_2023_02 = df_since_2023_02.withColumn(\"PULocationID\", df_since_2023_02[\"PULocationID\"].cast(\"long\"))\n",
    "df_since_2023_02 = df_since_2023_02.withColumn(\"DOLocationID\", df_since_2023_02[\"DOLocationID\"].cast(\"long\"))\n",
    "print(\"Count before 2023-02: \", df_before_2023_02.count())\n",
    "print(\"Count since 2023-02: \", df_since_2023_02.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_records = df_before_2023_02.union(df_since_2023_02)\n",
    "print(\"Total counts after union: \", df_records.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Transformation\n",
    "\n",
    "In this seciton, we did the following data transformation:\n",
    "\n",
    "- The column `request_datetime` contains null, we fill it with the `pickup_datetime`.\n",
    "- The column `congestion_surcharge` contains null value, we fill it with 0.\n",
    "- The column `trip_time` is null, we fill it with the difference between `dropoff_datetime` and `pickup_datetime`.\n",
    "- Add a column `pickup_time`, indicates how long it takes from the request to pickup.\n",
    "- Convert the `hvfhs_license_num` to the rider name, Juno, Uber, Via, Lyft.\n",
    "- Add columns `year`, `month`, `day`, `hour`, `weekday_n` based on the `request_datetime` for further partition or analysis.\n",
    "- Add a column `requset_hour`, indicates the hour of the request in \"yyyy-MM-dd HH:00:00\" format.\n",
    "- Convert the `trip_miles` from miles to kilometers.\n",
    "\n",
    "The `hvfhs_license_num` is the TLC license number of the HVFHS base or business. Convert it to the well-recognized name as following.\n",
    "- HV0002: Juno\n",
    "- HV0003: Uber\n",
    "- HV0004: Via\n",
    "- HV0005: Lyft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the request_datetime with pickup_datetime if null\n",
    "df_records = df_records.withColumn(\"request_datetime\", coalesce(df_records[\"request_datetime\"], df_records[\"pickup_datetime\"]))\n",
    "\n",
    "# if the value of congestion_surcharge is null, fill it with 0\n",
    "df_records = df_records.fillna({'congestion_surcharge': 0.0})\n",
    "print(\"Total counts after filling null: \", df_records.count())\n",
    "\n",
    "# Add column `trip_time` from `dropoff_datetime` and `pickup_datetime`.\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "df_records = df_records.withColumn(\"trip_time\", (unix_timestamp(df_records[\"dropoff_datetime\"]) - unix_timestamp(df_records[\"pickup_datetime\"])).cast(\"double\"))\n",
    "\n",
    "# add pickup_time\n",
    "df_records = df_records.withColumn(\"pickup_time\", (unix_timestamp(df_records[\"pickup_datetime\"]) - unix_timestamp(df_records[\"request_datetime\"]) ).cast(\"double\"))\n",
    "\n",
    "if not is_glue_job():\n",
    "    df_records.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column to indicate the rider\n",
    "df_records = df_records.withColumn(\"rider\", \n",
    "                        when(df_records[\"hvfhs_license_num\"] == \"HV0002\", \"Juno\")\n",
    "                        .when(df_records[\"hvfhs_license_num\"] == \"HV0003\", \"Uber\")\n",
    "                        .when(df_records[\"hvfhs_license_num\"] == \"HV0004\", \"Via\")\n",
    "                        .when(df_records[\"hvfhs_license_num\"] == \"HV0005\", \"Lyft\")\n",
    "                        .otherwise(\"Unknown\")  # Optional: for unmapped values\n",
    "                        )\n",
    "\n",
    "# drop the column\n",
    "df_records = df_records.drop(\"hvfhs_license_num\")\n",
    "\n",
    "print(\"Total counts after adding rider: \", df_records.count())\n",
    "\n",
    "if not is_glue_job():\n",
    "    df_records.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns `year`, `month`, `day`, `hour`, `weekday` from `request_datetime`.\n",
    "df_records = df_records.withColumn(\"year\", year(df_records[\"request_datetime\"]))\n",
    "df_records = df_records.withColumn(\"month\", month(df_records[\"request_datetime\"]))\n",
    "df_records = df_records.withColumn(\"day\", dayofmonth(df_records[\"request_datetime\"]))\n",
    "df_records = df_records.withColumn(\"hour\", hour(df_records[\"request_datetime\"]))\n",
    "df_records = df_records.withColumn(\"weekday_n\", dayofweek(df_records[\"request_datetime\"]))\n",
    "\n",
    "# Add a new column to indicate the hour of the request, example, 2019-01-02 03:00:00\n",
    "df_records = df_records.withColumn(\"request_hour\", date_format(df_records[\"request_datetime\"], \"yyyy-MM-dd HH:00:00\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the trip_miles from miles to kilometers\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "df_records = df_records.withColumn(\"trip_km\", round(df_records[\"trip_miles\"] * 1.60934, 2))\n",
    "\n",
    "# drop duplicated column\n",
    "df_records = df_records.drop(\"trip_miles\")\n",
    "\n",
    "if not is_glue_job():\n",
    "    df_records.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the final schema of the DataFrame\n",
    "df_records.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Public holidays in New York City\n",
    "\n",
    "We used ChatGPT to get the public holidays in New York City in from 2019 ~ 2023. The following is the code we used to get the public holidays, and ask it to output in CSV format. And save the result, and upload it to the S3 bucket.\n",
    "\n",
    "```\n",
    "List all public holidays in New York from 2019 to 2023. Output in CSV format. Here is an example:\n",
    "\n",
    "<example>\n",
    "Year,Month,Day,Holiday\n",
    "2019,1,1,New Year's Day\n",
    "2019,1,21,Martin Luther King Jr. Day\n",
    "</example>\n",
    "```\n",
    "\n",
    "Command to upload to the Amazon S3 bucket.\n",
    "```\n",
    "aws s3 cp ~/Developer/sjtu/data-analytics/data/holidays_ny.csv s3://qiaoshi-aws-ml/tlc/holidays/ny.csv\n",
    "```\n",
    "\n",
    "\n",
    "### 3.1 Read the Holiday Data from S3 bucket\n",
    "\n",
    "The holiday data is stored in the S3 bucket, we read the data into a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "holiday_schema = StructType([\n",
    "    StructField(\"year\", StringType()),\n",
    "    StructField(\"month\", StringType()),\n",
    "    StructField(\"day\", StringType()),\n",
    "    StructField(\"holiday\", StringType()),\n",
    "])\n",
    "\n",
    "# Read the CSV file from S3 into a DataFrame\n",
    "df_holidays = spark.read.schema(holiday_schema).csv(\"s3://qiaoshi-aws-ml/tlc/holidays/ny.csv\", header=True)\n",
    "\n",
    "# Show the first 5 rows of the DataFrame\n",
    "if not is_glue_job():\n",
    "    df_holidays.show(5)\n",
    "\n",
    "print(\"Total counts of holidays: \", df_holidays.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TLC Trip Data Record Enrichment\n",
    "\n",
    "In this section, we will enrich the TLC Trip Data Record with the public holidays data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Enrich with Publich Holiday Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the TLC Trip Data Record with the public holiday data\n",
    "df_enriched = df_records.join(df_holidays, on=['year', 'month', 'day'], how='left')\n",
    "\n",
    "# Add a column is_holiday of type boolean\n",
    "df_enriched = df_enriched.withColumn(\"is_holiday\", df_enriched[\"holiday\"].isNotNull())\n",
    "\n",
    "# drop the holiday date\n",
    "df_enriched = df_enriched.drop(\"holiday\")\n",
    "\n",
    "print(\"Add column is_holiday, and drop column holiday done.\")\n",
    "\n",
    "if not is_glue_job():\n",
    "    df_enriched.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the Enriched data to S3\n",
    "\n",
    "In this section, we will save the processed data to S3 in Parquet format, and partioned by year and rider. The partioned data will be used in the visualization tool, and machine learning. And it is good for parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result to S3 in parquet format, partitioned by year, month, rider\n",
    "\n",
    "if is_glue_job():\n",
    "    df_enriched.write.mode(\"overwrite\").partitionBy(\"year\", \"month\", \"rider\").parquet(\"s3://qiaoshi-aws-ml/tlc/results/full/trips/\")\n",
    "else:\n",
    "    df_enriched.write.mode(\"overwrite\").partitionBy(\"year\", \"month\", \"rider\").parquet(\"s3://qiaoshi-aws-ml/tlc/results/sample/trips/\")\n",
    "\n",
    "print(\"Output the enriched data to S3 done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Aggregation\n",
    "\n",
    "We will group the data by year, month, rider, pickup location, and aggreated to calculate the average pickup time, average cost per kilometers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Aggregate the data by year, month, rider, pickup location\n",
    "\n",
    "In this section, we will aggregate the data by year, month, rider, pickup location, and calculate the average pickup time, average cost per kilometers, and etc.\n",
    "\n",
    "- avg_pickup_time\n",
    "- avg_base_fare_per_km\n",
    "- avg_tips_per_km\n",
    "- avg_driver_pay_per_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, col, avg, round\n",
    "\n",
    "df_grouped_by_month = df_enriched.groupBy(\"rider\", \"year\", \"month\", \"PULocationID\").agg(\n",
    "    round(avg(\"pickup_time\"), 2).alias(\"avg_pickup_time\"),\n",
    "    round(avg(col(\"base_passenger_fare\") / col(\"trip_km\")), 2).alias(\"avg_base_fare_per_km\"),\n",
    "    round(avg(col(\"tips\") / col(\"trip_km\")), 2).alias(\"avg_tips_per_km\"),\n",
    "    round(avg(col(\"driver_pay\") / col(\"trip_km\")), 2).alias(\"avg_driver_pay_per_km\"),\n",
    ")\n",
    "\n",
    "print(\"The df_grouped_by_month count is: \", df_grouped_by_month.count())\n",
    "\n",
    "if not is_glue_job():\n",
    "    df_grouped_by_month.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the result to the S3 bucket\n",
    "if not is_glue_job():\n",
    "    df_grouped_by_month.write.mode(\"overwrite\").csv(\"s3://qiaoshi-aws-ml/tlc/results/sample/avg_by_month/\", header=True)\n",
    "else:\n",
    "    df_grouped_by_month.write.mode(\"overwrite\").csv(\"s3://qiaoshi-aws-ml/tlc/results/full/avg_by_month/\", header=True)\n",
    "\n",
    "print(\"Output the df_grouped_by_month to S3 done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Aggregate the data by weekday_n, rider, PUlocationID\n",
    "\n",
    "We aggregte the data by weekday_n, rider, PUlocationID, and calculate the average pickup time, average cost per kilometers, and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_by_weekday = df_enriched.filter(df_enriched[\"is_holiday\"] == False).groupBy(\"rider\", \"weekday_n\", \"PULocationID\").agg(\n",
    "    round(avg(\"pickup_time\"), 2).alias(\"avg_pickup_time\"),\n",
    "    round(avg(col(\"base_passenger_fare\") / col(\"trip_km\")), 2).alias(\"avg_base_fare_per_km\"),\n",
    "    round(avg(col(\"tips\") / col(\"trip_km\")), 2).alias(\"avg_tips_per_km\"),\n",
    "    round(avg(col(\"driver_pay\") / col(\"trip_km\")), 2).alias(\"avg_driver_pay_per_km\"),\n",
    ")\n",
    "\n",
    "print(\"The df_grouped_by_weekday count is: \", df_grouped_by_weekday.count())\n",
    "\n",
    "if not is_glue_job():\n",
    "    df_grouped_by_weekday.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_glue_job():\n",
    "    df_grouped_by_weekday.write.mode(\"overwrite\").csv(\"s3://qiaoshi-aws-ml/tlc/results/sample/avg_by_weekday/\", header=True)\n",
    "else:\n",
    "    df_grouped_by_weekday.write.mode(\"overwrite\").csv(\"s3://qiaoshi-aws-ml/tlc/results/full/avg_by_weekday/\", header=True)\n",
    "\n",
    "print(\"Output the df_grouped_by_weekday to S3 done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Analyze the total trips, total kilometers, total cost, total tips, total driver pay, total drivers\n",
    "\n",
    "In this section, we aggregate the data to output, and group by rider, year, month\n",
    "\n",
    "- sum_trip_time\n",
    "- sum_base_fare\n",
    "- sum_sales_tax\n",
    "- sum_tips\n",
    "- sum_driver_pay\n",
    "- sum_trip_km\n",
    "- distinct_drivers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count_distinct, round, col, date_format, concat, lit\n",
    "\n",
    "df_grouped_by_rider = df_enriched.groupBy(\"rider\", \"year\", \"month\").agg(\n",
    "    round((sum(\"trip_time\") / 60 / 60), 2).alias(\"sum_trip_time_hour\"),\n",
    "    round(sum(\"base_passenger_fare\"), 2).alias(\"sum_base_fare\"),\n",
    "    round(sum(\"sales_tax\"), 2).alias(\"sum_sales_tax\"),\n",
    "    round(sum(\"tips\"), 2).alias(\"sum_tips\"),\n",
    "    round(sum(\"driver_pay\"), 2).alias(\"sum_driver_pay\"),\n",
    "    round(sum(\"trip_km\"), 2).alias(\"sum_trip_km\"),\n",
    "    round(sum(\"base_passenger_fare\") - sum(\"driver_pay\"), 2).alias(\"sum_rider_income\"), # base_passenger_fare - driver_pay\n",
    "    round((sum(\"driver_pay\") + sum(\"tips\")) / sum(\"trip_km\"), 2).alias(\"avg_driver_income_per_km\"), # driver_pay + tips / trip_km\n",
    "    count_distinct(\"dispatching_base_num\").alias(\"distinct_drivers\")\n",
    ").withColumn(\"year_month\", date_format(concat(col(\"year\"), lit(\"-\"), col(\"month\"), lit(\"-01\")), \"yyyy-MM-dd\"))\n",
    "\n",
    "print(\"The df_grouped_by_rider count is: \", df_grouped_by_rider.count())\n",
    "\n",
    "if not is_glue_job():\n",
    "    df_grouped_by_rider.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the result to the S3 bucket\n",
    "\n",
    "if not is_glue_job():\n",
    "    df_grouped_by_rider.write.mode(\"overwrite\").csv(\"s3://qiaoshi-aws-ml/tlc/results/sample/group_by_rider/\", header=True)\n",
    "else:\n",
    "    df_grouped_by_rider.write.mode(\"overwrite\").csv(\"s3://qiaoshi-aws-ml/tlc/results/full/grup_by_rider/\", header=True)\n",
    "\n",
    "print(\"Output the df_grouped_by_rider to S3 done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

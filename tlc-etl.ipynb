{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TLC Trip Data Record Analysis\n",
    "\n",
    "This is the notebook used to analyze the TLC Trip Data Record. In this notebook, we will enrich the data with the New York weather data, public holiday data, drop unnecessary columns, and create new columns for further analysis.\n",
    "\n",
    "We will save the data in Amazon S3 bucket partioned by year, month, day, hour and vehicle operater. And prepare the data to be used in visualization tool, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Print the magics in Glue Spark kernel\n",
    "%help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Launch the Glue Interactive Sessions development environment\n",
    "\n",
    "To use this notebook, you must run the AWS Glue environment.\n",
    "\n",
    "We develop the notebook in the Glue Interactive Sessions development environment. To launch the environment, follow the steps below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%idle_timeout 2880\n",
    "%glue_version 4.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "# pyspark.sql.functions\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, dayofweek, date_format\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Detect if the code is running in a Glue job\n",
    "def is_glue_job():\n",
    "    try:\n",
    "        args = getResolvedOptions(sys.argv,['JOB_NAME'])\n",
    "        print(\"JOB_NAME: \", args['JOB_NAME'])\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "print(\"Running in Glue Job: \", is_glue_job())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TLC Trip Data Record \n",
    "\n",
    "The TLC Trip Data Record is a public dataset provided by the New York City Taxi and Limousine Commission (TLC) that contains data on over 1.1 billion taxi trips in New York City. The data is stored in Amazon S3 as a CSV file with a separate file for each month and year. The data is partitioned by year and month. The data is available from 2009 to the present. The data is updated on a monthly basis.\n",
    "\n",
    "Before we start, we have already download the For-hire Vehicle (FHV) trip records from 2019.2 to 2013.6. The data is stored in the S3 bucket.\n",
    "\n",
    "### 2.1 Load Trip Record Data\n",
    "\n",
    "Load the data from S3 bucket and convert to a Spark DataFrame. The Parquet schema since 2023-02 has changed, the PULocationID & DOLocationID was INT64 before 2023-02, but INT32 after 2023-02. To make the data consistent, we will convert to INT64.\n",
    "\n",
    "To to this, we put the data in two folders in Amazon S3 bucket, `fhvhv` contains the data before 2023-02, and `fhvhv_abnormal` contains the data since 2023-02. We load the data into 2 Spark DataFrame, cast the INT32 columns to INT64, concatenate the two DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, IntegerType, BinaryType, TimestampType, LongType\n",
    "\n",
    "schema_since_2023_02 = StructType([\n",
    "    StructField(\"hvfhs_license_num\", StringType()),\n",
    "    StructField(\"dispatching_base_num\", StringType()),\n",
    "    StructField(\"request_datetime\", TimestampType()),\n",
    "    StructField(\"pickup_datetime\", TimestampType()),\n",
    "    StructField(\"dropoff_datetime\", TimestampType()),\n",
    "    StructField(\"PULocationID\", IntegerType()),\n",
    "    StructField(\"DOLocationID\", IntegerType()),\n",
    "    StructField(\"trip_miles\", DoubleType()),\n",
    "    StructField(\"trip_time\", LongType()),\n",
    "    StructField(\"base_passenger_fare\", DoubleType()),\n",
    "    StructField(\"tolls\", DoubleType()),\n",
    "    StructField(\"sales_tax\", DoubleType()),\n",
    "    StructField(\"congestion_surcharge\", DoubleType()),\n",
    "    StructField(\"tips\", DoubleType()),\n",
    "    StructField(\"driver_pay\", DoubleType()),\n",
    "    StructField(\"shared_request_flag\", StringType()),\n",
    "    StructField(\"shared_match_flag\", StringType())\n",
    "])\n",
    "\n",
    "schema_before_2023_02 = StructType([\n",
    "    StructField(\"hvfhs_license_num\", StringType()),\n",
    "    StructField(\"dispatching_base_num\", StringType()),\n",
    "    StructField(\"request_datetime\", TimestampType()),\n",
    "    StructField(\"pickup_datetime\", TimestampType()),\n",
    "    StructField(\"dropoff_datetime\", TimestampType()),\n",
    "    StructField(\"PULocationID\", LongType()),\n",
    "    StructField(\"DOLocationID\", LongType()),\n",
    "    StructField(\"trip_miles\", DoubleType()),\n",
    "    StructField(\"trip_time\", LongType()),\n",
    "    StructField(\"base_passenger_fare\", DoubleType()),\n",
    "    StructField(\"tolls\", DoubleType()),\n",
    "    StructField(\"sales_tax\", DoubleType()),\n",
    "    StructField(\"congestion_surcharge\", DoubleType()),\n",
    "    StructField(\"tips\", DoubleType()),\n",
    "    StructField(\"driver_pay\", DoubleType()),\n",
    "    StructField(\"shared_request_flag\", StringType()),\n",
    "    StructField(\"shared_match_flag\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# if running in Glue job, read all data, otherwise, read a sample\n",
    "if is_glue_job():\n",
    "    print(\"Running in Glue job, read all data\")\n",
    "    df_before_2023_02 = spark.read.schema(schema_before_2023_02).parquet(\"s3://qiaoshi-aws-ml/tlc/fhvhv/\")\n",
    "    df_since_2023_02 = spark.read.schema(schema_since_2023_02).parquet(\"s3://qiaoshi-aws-ml/tlc/fhvhv_abnormal/\")\n",
    "else:\n",
    "    print(\"Running in a development environment, read a sample\")\n",
    "    df_before_2023_02 = spark.read.schema(schema_before_2023_02).parquet(\"s3://qiaoshi-aws-ml/tlc/fhvhv/\").sample(False, 0.01) # 1% sample\n",
    "    df_since_2023_02 = spark.read.schema(schema_since_2023_02).parquet(\"s3://qiaoshi-aws-ml/tlc/fhvhv_abnormal/\").sample(False, 0.01) # 1% sample\n",
    "\n",
    "df_since_2023_02 = df_since_2023_02.withColumn(\"PULocationID\", df_since_2023_02[\"PULocationID\"].cast(\"long\"))\n",
    "df_since_2023_02 = df_since_2023_02.withColumn(\"DOLocationID\", df_since_2023_02[\"DOLocationID\"].cast(\"long\"))\n",
    "print(\"Count before 2023-02: \", df_before_2023_02.count())\n",
    "print(\"Count since 2023-02: \", df_since_2023_02.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df_records = df_before_2023_02.union(df_since_2023_02)\n",
    "print(\"Total counts after union: \", df_records.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Transformation\n",
    "\n",
    "In this seciton, we did the following data transformation:\n",
    "\n",
    "- The column `request_datetime` contains null, we fill it with the `pickup_datetime`.\n",
    "- The column `congestion_surcharge` contains null value, we fill it with 0.\n",
    "- The column `trip_time` is null, we fill it with the difference between `dropoff_datetime` and `pickup_datetime`.\n",
    "- Convert the `hvfhs_license_num` to the rider name, Juno, Uber, Via, Lyft.\n",
    "- Convert the U.S metrics to metric system, so it is easier to understand for international audience.\n",
    "- Add columns `year`, `month`, `day`, `hour`, `weekday` based on the `request_datetime` for further partitioning the data. We will use this to join with the weather data in New York City.\n",
    "- Convert the `trip_miles` from miles to kilometers.\n",
    "\n",
    "\n",
    "The `hvfhs_license_num` is the TLC license number of the HVFHS base or business. Convert it to the well-recognized name as following.\n",
    "- HV0002: Juno\n",
    "- HV0003: Uber\n",
    "- HV0004: Via\n",
    "- HV0005: Lyft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# fill the request_datetime with pickup_datetime if null\n",
    "df_records = df_records.withColumn(\"request_datetime\", coalesce(df_records[\"request_datetime\"], df_records[\"pickup_datetime\"]))\n",
    "\n",
    "# if the value of congestion_surcharge is null, fill it with 0\n",
    "df_records = df_records.fillna({'congestion_surcharge': 0.0})\n",
    "print(\"Total counts after filling null: \", df_records.count())\n",
    "\n",
    "df_records.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Add column `trip_time` from `dropoff_datetime` and `pickup_datetime`.\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "df_records = df_records.withColumn(\"trip_time\", (unix_timestamp(df_records[\"dropoff_datetime\"]) - unix_timestamp(df_records[\"pickup_datetime\"])).cast(\"double\"))\n",
    "print(\"Total counts after adding trip_time: \", df_records.count())\n",
    "df_records.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# add a new column to indicate the rider\n",
    "df_records = df_records.withColumn(\"rider\", \n",
    "                        when(df_records[\"hvfhs_license_num\"] == \"HV0002\", \"Juno\")\n",
    "                        .when(df_records[\"hvfhs_license_num\"] == \"HV0003\", \"Uber\")\n",
    "                        .when(df_records[\"hvfhs_license_num\"] == \"HV0004\", \"Via\")\n",
    "                        .when(df_records[\"hvfhs_license_num\"] == \"HV0005\", \"Lyft\")\n",
    "                        .otherwise(\"Unknown\")  # Optional: for unmapped values\n",
    "                        )\n",
    "print(\"Total counts after adding rider: \", df_records.count())\n",
    "df_records.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Add columns `year`, `month`, `day`, `hour`, `weekday` from `request_datetime`.\n",
    "df_records = df_records.withColumn(\"year\", year(df_records[\"request_datetime\"]))\n",
    "df_records = df_records.withColumn(\"month\", month(df_records[\"request_datetime\"]))\n",
    "df_records = df_records.withColumn(\"day\", dayofmonth(df_records[\"request_datetime\"]))\n",
    "df_records = df_records.withColumn(\"hour\", hour(df_records[\"request_datetime\"]))\n",
    "df_records = df_records.withColumn(\"weekday_n\", dayofweek(df_records[\"request_datetime\"]))\n",
    "df_records = df_records.withColumn(\"weekday\", date_format(df_records[\"request_datetime\"], \"EEEE\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the trip_miles from miles to kilometers\n",
    "df_records = df_records.withColumn(\"trip_km\", df_records[\"trip_miles\"] * 1.60934)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Write the data to the S3 bucket in parquet format partitioned by year, month, day if running in Glue job\n",
    "if is_glue_job():\n",
    "    print(\"Writing to S3 bucket...\")\n",
    "    # patition by day has too many files, we partition by year and month, rider\n",
    "    df_records.write.mode(\"append\").format(\"parquet\").partitionBy(\"year\", \"month\", \"day\").save(\"s3://qiaoshi-aws-ml/tlc/fhvhv_partitioned/\")\n",
    "    print(\"Done writing to S3 bucket.\")\n",
    "else:\n",
    "    print(\"Not running in Glue job, skip writing to S3 bucket.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. New York Weather data\n",
    "\n",
    "Weather is a very important factor of the taxi trip. We use the NY_Weather data to join with the TLC Trip Data Record. The data is downloaded from [Visual Crossing](https://www.visualcrossing.com/). \n",
    "\n",
    "We download the weather data from 2019.1 to 2023.6, and store the data in the Amazon S3 bucket. After that, we use the data to join with the TLC Trip Data Record.\n",
    "\n",
    "### 3.1 Data Column Definitions\n",
    "\n",
    "- name: name of the weather station\n",
    "- datetime: date and time of the weather record\n",
    "- temp: temperature in Fahrenheit\n",
    "- feelslike: feels like temperature in Fahrenheit\n",
    "- dewp: dew point in Fahrenheit\n",
    "- humid: relative humidity\n",
    "- precip: precipitation in inches\n",
    "- precipprob: probability of precipitation\n",
    "- snow: snowfall in inches\n",
    "- snowdepth: snow depth in inches\n",
    "- windgust: wind gust in miles per hour\n",
    "- windspeed: wind speed in miles per hour\n",
    "- winddir: wind direction in degrees\n",
    "- sealvlpressure: sea level pressure in millibars\n",
    "- cloudcover: cloud cover percentage\n",
    "- visibility: visibility in miles\n",
    "- solarradiation: solar radiation in watts per square meter\n",
    "- solarenergy: solar energy in megajoules per square meter\n",
    "- uvindex: UV index\n",
    "- severisky: severe weather risk index\n",
    "- conditions: weather conditions\n",
    "- icon: weather icon\n",
    "- stations: number of weather stations reporting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load Weather Data\n",
    "\n",
    "Since the weather data is well formated, we use Glue Crawler to create a Glue Table, and load into a Spark DataFrame.\n",
    "\n",
    "The weather data is stored in the Amazon S3 bucket, and the Glue Table name is `tlcny_weather`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dyf_weather = glueContext.create_dynamic_frame.from_catalog(database='tlc', table_name='tlcny_weather')\n",
    "dyf_weather.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Convert Dynamic DataFrame to Spark DataFrame\n",
    "df_weather = dyf_weather.toDF()\n",
    "print(\"Count of weather records: \", df_weather.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Transformation\n",
    "\n",
    "In this seciton, we did the following data transformation:\n",
    "\n",
    "1. Drop duplicated fields `name`, `dew`, `stations`, `conditions`, `severerisk`.\n",
    "2. Convert the U.S metrics to metric system, so it is easier to understand for international audience.\n",
    "3. Add columns `year`, `month`, `day`, `hour`.\n",
    "4. Remove duplicated rows.\n",
    "\n",
    "We added the `year`, `month`, `day`, `hour` fields based on the `request_datetime` in the TLC Trip Data Record.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Drop columns: name (location name), stations (number of stations reporting), conditions (weather conditions), severerisk (severe weather risk)\n",
    "df_weather = df_weather.drop(\"name\", \"dew\", \"stations\", \"conditions\", \"severerisk\")\n",
    "\n",
    "df_weather.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Convert the U.S metrics to metric system, so it is easier to understand for international audience\n",
    "df_weather = df_weather.withColumn(\"temp_c\", round((df_weather[\"temp\"] - 32) * 5/9, 1))\n",
    "df_weather = df_weather.withColumn(\"feelslike_c\", round((df_weather[\"feelslike\"] - 32) * 5/9, 1))\n",
    "df_weather = df_weather.withColumn(\"precip_cm\", round(df_weather[\"precip\"] * 0.3048 * 100, 1)) # centimeters\n",
    "df_weather = df_weather.withColumn(\"snow_cm\", round(df_weather[\"snow\"] * 0.3048 * 100, 1)) # centimeters\n",
    "df_weather = df_weather.withColumn(\"snowdepth_cm\", round(df_weather[\"snowdepth\"] * 0.3048 * 100, 1)) #centimeters\n",
    "df_weather = df_weather.withColumn(\"windgust_mps\", round(df_weather[\"windgust\"] * 0.44704, 1))  # meters per second\n",
    "df_weather = df_weather.withColumn(\"windspeed_mps\", round(df_weather[\"windspeed\"] * 0.44704, 1)) # meters per second\n",
    "df_weather = df_weather.withColumn(\"visibility_km\", round(df_weather[\"visibility\"] * 1.60934, 1)) # kilometers\n",
    "\n",
    "df_weather.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Add columns year, month, day, hour based on field datetime\n",
    "\n",
    "df_weather = df_weather.withColumn(\"year\", year(df_weather[\"datetime\"]))\n",
    "df_weather = df_weather.withColumn(\"month\", month(df_weather[\"datetime\"]))\n",
    "df_weather = df_weather.withColumn(\"day\", dayofmonth(df_weather[\"datetime\"]))\n",
    "df_weather = df_weather.withColumn(\"hour\", hour(df_weather[\"datetime\"]))\n",
    "\n",
    "df_weather.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "duplicated_rows = df_weather.groupBy(\"year\", \"month\", \"day\", \"hour\").count().filter(col(\"count\") > 1)\n",
    "duplicated_rows.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# There is duplicated records in weather data, remove the duplicated records\n",
    "print(\"Count before removing duplicated records: \", df_weather.count())\n",
    "df_weather = df_weather.dropDuplicates([\"year\", \"month\", \"day\", \"hour\"])\n",
    "print(\"Count after removing duplicated records: \", df_weather.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Public holidays in New York City\n",
    "\n",
    "We used ChatGPT to get the public holidays in New York City in from 2019 ~ 2023. The following is the code we used to get the public holidays, and ask it to output in CSV format. And save the result, and upload it to the S3 bucket.\n",
    "\n",
    "```\n",
    "List all public holidays in New York from 2019 to 2023. Output in CSV format. Here is an example:\n",
    "\n",
    "<example>\n",
    "Year,Month,Day,Holiday\n",
    "2019,1,1,New Year's Day\n",
    "2019,1,21,Martin Luther King Jr. Day\n",
    "</example>\n",
    "```\n",
    "\n",
    "Command to upload to the Amazon S3 bucket, and use AWS Glue Crawler to create a table\n",
    "```\n",
    "aws s3 cp ~/Developer/sjtu/data-analytics/data/holidays_ny.csv s3://qiaoshi-aws-ml/tlc/holidays/ny.csv\n",
    "```\n",
    "\n",
    "\n",
    "### 3.1 Read the NY_Weather data from S3 bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dyf_holidays = glueContext.create_dynamic_frame.from_catalog(database='tlc', table_name='holidays')\n",
    "\n",
    "# Convert Dynamic DataFrame to Spark DataFrame\n",
    "df_holidays = dyf_holidays.toDF()\n",
    "\n",
    "df_holidays.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TLC Trip Data Record Enrichment, Cleanup & Exploration\n",
    "\n",
    "In this section, we will enrich the TLC Trip Data Record with the weather data and public holidays data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Enrich with Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Join the TLC Trip Data Record with the weather data\n",
    "print(\"total records before join with weather data: \", df_records.count())\n",
    "df_enriched = df_records.join(df_weather, on=['year', 'month', 'day', 'hour'], how='left')\n",
    "\n",
    "\n",
    "print(\"total records after join with weather data: \", df_enriched.count())\n",
    "df_enriched.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Enrich with Publich Holiday Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Join the TLC Trip Data Record with the public holiday data\n",
    "df_enriched = df_enriched.join(df_holidays, on=['year', 'month', 'day'], how='left').dropDuplicates()\n",
    "print(\"total records after join with holiday data: \", df_enriched.count())\n",
    "\n",
    "# Add a column is_holiday of type boolean\n",
    "df_enriched = df_enriched.withColumn(\"is_holiday\", df_enriched[\"holiday\"].isNotNull())\n",
    "print(\"total records after adding is_holiday column: \", df_enriched.count())\n",
    "\n",
    "df_enriched.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Data Cleanup\n",
    "\n",
    "There is duplicated columns in the DataFrame, we will remove some duplicated records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# drop duplicated columns\n",
    "df_enriched = df_enriched.drop(\"hvfhs_license_num\", \"datetime\", \"temp\", \"feelslike\", \"precip\", \"snow\", \"snowdepth\", \"windgust\", \"windspeed\", \"visibility\", \"trip_miles\")\n",
    "\n",
    "df_enriched.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Exploration\n",
    "\n",
    "We conduct a draft analysis of the data. We will use the data to answer the following questions:\n",
    "\n",
    "- Is there any relevance between the weather and the taxi trip?\n",
    "- What is the relevance between each columns?\n",
    "- What is the busiest day of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Column Relevance\n",
    "\n",
    "Find the relevance among each columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Compute the correlation matrix\n",
    "# corr = df_enriched.corr()\n",
    "\n",
    "# # Plot the correlation matrix as a heatmap\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# sns.heatmap(corr, cmap='coolwarm', annot=True, fmt=\".2f\")\n",
    "# plt.title(\"Correlation Matrix\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 XXXX\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the enriched TLC Trip Data Record\n",
    "\n",
    "After the ETL, we save the enriched TLC Trip Data Record to the S3 bucket. We will use the data to visualize the data in BI tools (e.g., Amazon QuickSight), and build the machine learning model in the Amazon SageMaker.\n",
    "\n",
    "We will try both zero-code machine learning using Amazon SageMaker Canvas, and code-based machine learning using Amazon SageMaker Studio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Count of records before saving: \", df_enriched.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Save the data to Amazon S3 bucket, partitioned by year, month, day\n",
    "df_enriched.write.partitionBy(\"year\", \"month\", \"day).parquet(\"s3://qiaoshi-aws-ml/tlc/fhvhv_final/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Commit the Job to AWS Glue, complete the job.\n",
    "job.commit()\n",
    "\n",
    "print('Data processing completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Finance Data\n",
    "\n",
    "Uber, and Lyft has been public companies for a while. We can get the finance data from the public market. We can use the finance data to analyze the financial performance of the companies, and compare with the TLC Trip Data Record.\n",
    "\n",
    "As of today, Uber, Lyft has never split the stock, Juno, Via are not public companies. We will use the stock price to analyze the financial performance of the companies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

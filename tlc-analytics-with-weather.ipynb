{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TLC Data Trip Data Analytics with Weather\n",
    "\n",
    "> Make sure you have save the trip data in S3 and create a Glue Data Catalog before start futher data processing.\n",
    "\n",
    "We are going to use the TLC Trip Record Data to analyze the trip data with weather data. And create a Machine Learning model to predict the trip requests in each hour and NY zone area.\n",
    "\n",
    "We will cover the following in this section:\n",
    "1. Enrich the trip data with weather data\n",
    "2. Create some analytics on the relevance between trip data and weather data\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# intialize the Glue environment\n",
    "\n",
    "%idle_timeout 2880\n",
    "%glue_version 4.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 32\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "\n",
    "import os\n",
    "# Detect if the code is running in a Glue job\n",
    "def is_glue_job():\n",
    "    try:\n",
    "        args = getResolvedOptions(sys.argv,['JOB_NAME'])\n",
    "        print(\"JOB_NAME: \", args['JOB_NAME'])\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "print(\"Running in Glue Job: \", is_glue_job())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. New York Weather data\n",
    "\n",
    "Weather is a very important factor of the taxi trip. We use the NY_Weather data to join with the TLC Trip Data Record. The data is downloaded from [Visual Crossing](https://www.visualcrossing.com/). \n",
    "\n",
    "We download the weather data from 2019.1 to 2023.6, and store the data in the Amazon S3 bucket. After that, we use the data to join with the TLC Trip Data Record.\n",
    "\n",
    "\n",
    "**Column Description**\n",
    "- name: name of the weather station\n",
    "- datetime: date and time of the weather record\n",
    "- temp: temperature in Fahrenheit\n",
    "- feelslike: feels like temperature in Fahrenheit\n",
    "- dewp: dew point in Fahrenheit\n",
    "- humid: relative humidity\n",
    "- precip: precipitation in inches\n",
    "- precipprob: probability of precipitation\n",
    "- snow: snowfall in inches\n",
    "- snowdepth: snow depth in inches\n",
    "- windgust: wind gust in miles per hour\n",
    "- windspeed: wind speed in miles per hour\n",
    "- winddir: wind direction in degrees\n",
    "- sealvlpressure: sea level pressure in millibars\n",
    "- cloudcover: cloud cover percentage\n",
    "- visibility: visibility in miles\n",
    "- solarradiation: solar radiation in watts per square meter\n",
    "- solarenergy: solar energy in megajoules per square meter\n",
    "- uvindex: UV index\n",
    "- severisky: severe weather risk index\n",
    "- conditions: weather conditions\n",
    "- icon: weather icon\n",
    "- stations: number of weather stations reporting data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Weather Data\n",
    "\n",
    "Since the weather data is well formated, we use Glue Crawler to create a Glue Table, and load into a Spark DataFrame.\n",
    "\n",
    "The weather data is stored in the Amazon S3 bucket, and the Glue Table name is `tlcny_weather`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Spark DataFrame from the Glue Catalog\n",
    "df_weather = glueContext.create_dynamic_frame.from_catalog(database = \"tlc\", table_name = \"tlcny_weather\").toDF()\n",
    "\n",
    "# print the schema if not running in a Glue job\n",
    "if not is_glue_job():\n",
    "    print(\"Schema before transformation:\")\n",
    "    df_weather.printSchema()\n",
    "\n",
    "# Count the number of rows\n",
    "print(\"Number of rows:\", df_weather.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Cleanup\n",
    "\n",
    "In this seciton, we did the following data transformation:\n",
    "\n",
    "1. Drop duplicated fields `name`, `dew`, `stations`, `conditions`, `severerisk`.\n",
    "2. Convert the U.S metrics to metric system, so it is easier to understand for international audience.\n",
    "3. Add columns `year`, `month`, `day`, `hour`.\n",
    "4. Remove duplicated rows.\n",
    "\n",
    "We added the `year`, `month`, `day`, `hour` fields based on the `request_datetime` in the TLC Trip Data Record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Drop columns: name (location name), stations (number of stations reporting), conditions (weather conditions), severerisk (severe weather risk)\n",
    "df_weather = df_weather.drop(\"name\", \"dew\", \"stations\", \"conditions\", \"severerisk\")\n",
    "\n",
    "if not is_glue_job():\n",
    "    df_weather.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Convert the U.S metrics to metric system, so it is easier to understand for international audience\n",
    "df_weather = df_weather.withColumn(\"temp_c\", round((df_weather[\"temp\"] - 32) * 5/9, 1))\n",
    "df_weather = df_weather.withColumn(\"feelslike_c\", round((df_weather[\"feelslike\"] - 32) * 5/9, 1))\n",
    "df_weather = df_weather.withColumn(\"precip_cm\", round(df_weather[\"precip\"] * 0.3048 * 100, 1)) # centimeters\n",
    "df_weather = df_weather.withColumn(\"snow_cm\", round(df_weather[\"snow\"] * 0.3048 * 100, 1)) # centimeters\n",
    "df_weather = df_weather.withColumn(\"snowdepth_cm\", round(df_weather[\"snowdepth\"] * 0.3048 * 100, 1)) #centimeters\n",
    "df_weather = df_weather.withColumn(\"windgust_mps\", round(df_weather[\"windgust\"] * 0.44704, 1))  # meters per second\n",
    "df_weather = df_weather.withColumn(\"windspeed_mps\", round(df_weather[\"windspeed\"] * 0.44704, 1)) # meters per second\n",
    "df_weather = df_weather.withColumn(\"visibility_km\", round(df_weather[\"visibility\"] * 1.60934, 1)) # kilometers\n",
    "\n",
    "df_weather = df_weather.drop(\"temp\", \"feelslike\", \"precip\", \"snow\", \"snowdepth\", \"windgust\", \"windspeed\", \"visibility\")\n",
    "\n",
    "if not is_glue_job():\n",
    "    df_weather.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Add columns year, month, day, hour based on field datetime\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour\n",
    "\n",
    "df_weather = df_weather.withColumn(\"year\", year(df_weather[\"datetime\"]))\n",
    "df_weather = df_weather.withColumn(\"month\", month(df_weather[\"datetime\"]))\n",
    "df_weather = df_weather.withColumn(\"day\", dayofmonth(df_weather[\"datetime\"]))\n",
    "df_weather = df_weather.withColumn(\"hour\", hour(df_weather[\"datetime\"]))\n",
    "\n",
    "if not is_glue_job():\n",
    "    df_weather.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "duplicated_rows = df_weather.groupBy(\"year\", \"month\", \"day\", \"hour\").count().filter(col(\"count\") > 1)\n",
    "\n",
    "# we found join with the trip data will have more records, this is usually caused by the weather \n",
    "# data contains multiple records for a data point\n",
    "duplicated_rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# There is duplicated records in weather data, remove the duplicated records\n",
    "print(\"Count before removing duplicated records: \", df_weather.count())\n",
    "\n",
    "df_weather = df_weather.dropDuplicates([\"year\", \"month\", \"day\", \"hour\"])\n",
    "\n",
    "print(\"Count after removing duplicated records: \", df_weather.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df_weather.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Check for null values in each column\n",
    "null_counts = df_weather.agg(*[sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_weather.columns])\n",
    "\n",
    "# Display the null counts\n",
    "null_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Fill with 0 if there is null value in the windgust_mps column\n",
    "df_weather = df_weather.fillna(0, subset=[\"windgust_mps\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Trip Data\n",
    "\n",
    "Load the trip data from the Glue Table `trips` into a Spark DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df_trips = glueContext.create_dynamic_frame.from_catalog(database = \"tlc\", table_name = \"trips\")\n",
    "\n",
    "if not is_glue_job():\n",
    "    df_trips = df_trips.toDF().sample(False, 0.002, 42) # sample 0.2% of the data\n",
    "    df_trips.printSchema()\n",
    "else:\n",
    "    df_trips = df_trips.toDF()\n",
    "\n",
    "print(\"Number of trips:\", df_trips.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Aggregate the Trip Data\n",
    "\n",
    "The task in this section is to generate a new DataFrame with aggregated criteria.\n",
    "\n",
    "- year\n",
    "- month\n",
    "- day\n",
    "- hour\n",
    "- PUlocationID\n",
    "- Count(*) as trip_needs\n",
    "- request_hour\n",
    "- is_holiday\n",
    "- weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df_trips_grouped = df_trips.groupBy(\"year\", \"month\", \"day\", \"hour\", \"PULocationID\").count().withColumnRenamed(\"count\", \"total_trips\")\n",
    "\n",
    "if not is_glue_job():\n",
    "    df_trips_grouped.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df_trips_grouped.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col, lit, concat\n",
    "\n",
    "df_trips_grouped = df_trips_grouped.withColumn(\"request_hour\", date_format(concat(col(\"year\"), lit(\"-\"), col(\"month\"), lit(\"-\"), col(\"day\"), lit(\" \"), col(\"hour\"), lit(\":00:00\")), \"yyyy-MM-dd HH:00:00\"))\n",
    "\n",
    "if not is_glue_job():\n",
    "    df_trips_grouped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofweek\n",
    "# add the column weekday_n\n",
    "\n",
    "df_trips_grouped = df_trips_grouped.withColumn(\"weekday_n\", dayofweek(df_trips_grouped[\"request_hour\"]))\n",
    "\n",
    "if not is_glue_job():\n",
    "    df_trips_grouped.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enrich the Trip Data \n",
    "Before we run this secion, make sure you have create a Glue Table for the TLC Trip Data Record.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Enrich with Holiday Data\n",
    "\n",
    "Add a column `is_holiday` to the DataFrame, and drop the holiday name column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df_holidays = glueContext.create_dynamic_frame.from_catalog(database = \"tlc\", table_name = \"holidays\").toDF()\n",
    "\n",
    "if not is_glue_job():\n",
    "    df_holidays.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Join the TLC Trip Data Record with the public holiday data\n",
    "df_trips_grouped = df_trips_grouped.join(df_holidays, on=['year', 'month', 'day'], how='left')\n",
    "\n",
    "# Add a column is_holiday of type boolean, and dop the column holiday\n",
    "df_trips_grouped = df_trips_grouped.withColumn(\"is_holiday\", df_trips_grouped[\"holiday\"].isNotNull())\n",
    "\n",
    "# drop the column holiday\n",
    "df_trips_grouped.drop(\"holiday\")\n",
    "\n",
    "print(\"Add column is_holiday, and drop column holiday done.\")\n",
    "\n",
    "if not is_glue_job():\n",
    "    df_trips_grouped.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Enrich with weather data\n",
    "\n",
    "The weather data is hourly, we enrich the trip data with weather data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# drop the duplicated columns first, because we are going to join with datetime field\n",
    "df_weather = df_weather.drop(\"year\", \"month\", \"day\", \"hour\")\n",
    "\n",
    "df_trips_grouped_with_weather = df_trips_grouped.join(df_weather, df_trips_grouped['request_hour'] == df_weather['datetime'], how='left')\n",
    "\n",
    "df_trips_grouped_with_weather = df_trips_grouped_with_weather.drop(\"datetime\")\n",
    "\n",
    "if not is_glue_job():\n",
    "    df_trips_grouped_with_weather.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save the data to S3\n",
    "\n",
    "Save the processed DataFrame to S3, for ML tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Output a partitioned records in parquet format\n",
    "\n",
    "df_trips_grouped_with_weather.write.mode(\"overwrite\").partitionBy(\"year\", \"month\").parquet(\"s3://qiaoshi-aws-ml/tlc/results/full/trips_with_weather/parquet/\")\n",
    "\n",
    "print(\"Write to S3 in parquet format done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# output the full dataset to S3\n",
    "df_trips_grouped_with_weather.write.mode(\"overwrite\").csv(\"s3://qiaoshi-aws-ml/tlc/results/full/trips_with_weather/csv/\", header=True)\n",
    "\n",
    "print(\"Output full records to S3 done.\")\n",
    "\n",
    "# output a sample dataset to S3\n",
    "df_trips_grouped_with_weather.sample(False, 0.01, 42).write.mode(\"overwrite\").csv(\"s3://qiaoshi-aws-ml/tlc/results/sample/trips_with_weather/csv/\", header=True)\n",
    "print(\"output sample records to S3 done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
